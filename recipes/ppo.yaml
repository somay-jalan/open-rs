# PPO Training Configuration
# Based on open-rs GRPO config, adapted for PPO

# Dataset configuration
dataset_name: "knoveleng/open-rs"
dataset_config: null
dataset_train_split: "train"
dataset_test_split: "test"

# Model configuration
model_name_or_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
torch_dtype: "bfloat16"
attn_implementation: "flash_attention_2"
model_revision: "main"
trust_remote_code: true

# Value model configuration (NEW for PPO)
# If null, uses same architecture as policy model
# For memory efficiency, you can use a smaller model
value_model_name_or_path: null  # or "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Reward functions
reward_funcs:
  - "accuracy"
  - "format"
  - "tag_count"
  # Optional: add more rewards
  # - "cosine"
  # - "repetition_penalty"
  # - "reasoning_steps"

# Cosine scaling parameters (if using cosine reward)
cosine_min_value_wrong: -1.0
cosine_max_value_wrong: -0.5
cosine_min_value_correct: 0.5
cosine_max_value_correct: 1.0
cosine_max_len: 3584

# Repetition penalty parameters (if using)
repetition_n_grams: 3
repetition_max_penalty: -1.0

# PPO-specific training arguments
output_dir: "models/open-rs-ppo"
logging_dir: "logs/open-rs-ppo"

# PPO hyperparameters (KEY DIFFERENCES FROM GRPO)
learning_rate: 1.0e-6
vf_coef: 0.1  # Value function loss coefficient
ppo_epochs: 4  # Number of PPO epochs per batch
mini_batch_size: 1  # Reduce for memory efficiency
batch_size: 64  # Total batch size
gradient_accumulation_steps: 8

# KL divergence control
init_kl_coef: 0.2
target_kl: 6.0
adap_kl_ctrl: true

# Generation parameters
num_generations: 4
temperature: 0.6
top_p: 0.95
max_new_tokens: 8192
response_length: 8192

# Training configuration
num_train_epochs: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1

# Optimization
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
weight_decay: 0.0
warmup_ratio: 0.1
max_grad_norm: 1.0

# Mixed precision
bf16: true
fp16: false

# Memory optimization (IMPORTANT for PPO)
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Logging and checkpointing
logging_steps: 1
save_strategy: "steps"
save_steps: 50
save_total_limit: 2
eval_strategy: "steps"
eval_steps: 100

# Hub configuration
push_to_hub: false
hub_model_id: null
hub_model_revision: "main"
push_to_hub_revision: false

# W&B logging
report_to: ["wandb"]
wandb_project: "open-rs-ppo"
wandb_entity: null

# System prompt (optional)
system_prompt: |
  You are a helpful AI assistant. Think step-by-step and provide clear reasoning.
  Format your response with:
  <think>
  Your reasoning process here
  </think>
  <answer>
  Your final answer here
  </answer>

# Seed for reproducibility
seed: 42

# PEFT/LoRA configuration (optional, for memory efficiency)
use_peft: false
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]